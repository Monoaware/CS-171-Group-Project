{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4eb08c",
   "metadata": {},
   "source": [
    "# **Model Training & Testing Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc6f32",
   "metadata": {},
   "source": [
    "The first step is to set up our notebook for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da83dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Prepare notebook for PyTorch:\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240a3de",
   "metadata": {},
   "source": [
    "Next, let's load in our dataset. Recall that we resized our images to be 224 by 224 pixels in the data preprocessing notebook. Because MLP models and CNN models require different types of input, the next code cell will just generically load in the data. We also need to make sure that the labels are displayed correctly to the models, so we'll format them like make_model (i.e., \"Toyota_Camry). This step probably should have been done in the data preprocessing notebook but it was a slight oversight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ed6cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for combining labels:\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Class that combines make & model names (like a custom ImageFolder):\n",
    "class MakeModelCombinedDataset(Dataset):\n",
    "    IMG_EXTS = {\".jpg\", \".jpeg\"}\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = []  # list of (path, class_name)\n",
    "        self.transform = transform\n",
    "\n",
    "        for make in sorted(os.listdir(root)):\n",
    "            make_dir = os.path.join(root, make)\n",
    "            if not os.path.isdir(make_dir):\n",
    "                continue\n",
    "            for model in sorted(os.listdir(make_dir)):\n",
    "                model_dir = os.path.join(make_dir, model)\n",
    "                if not os.path.isdir(model_dir):\n",
    "                    continue\n",
    "                class_name = f\"{make}_{model}\"\n",
    "                for fname in sorted(os.listdir(model_dir)):\n",
    "                    _, ext = os.path.splitext(fname)\n",
    "                    if ext.lower() in self.IMG_EXTS:\n",
    "                        self.samples.append((os.path.join(model_dir, fname), class_name))\n",
    "\n",
    "        classes = sorted({c for _, c in self.samples})\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, class_name = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.class_to_idx[class_name]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ba830e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 102\n",
      "Sample class labels: ['Audi_A3', 'Audi_A4', 'Audi_A6', 'BMW_1', 'BMW_3', 'BMW_5', 'Chevrolet_Aveo', 'Chevrolet_Cruze', 'Citroen_Berlingo', 'Citroen_C-Elysee']\n"
     ]
    }
   ],
   "source": [
    "# Imports for loading images:\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Common transformations (since we already did resizing, we just need to load the images onto Tensors and normalize them):\n",
    "common_transforms  = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Use ImageLoader:\n",
    "train_dataset = MakeModelCombinedDataset(root=\"Final_Dataset/train\", transform=common_transforms)\n",
    "val_dataset = MakeModelCombinedDataset(root=\"Final_Dataset/val\", transform=common_transforms)\n",
    "test_dataset = MakeModelCombinedDataset(root=\"Final_Dataset/test\", transform=common_transforms)\n",
    "\n",
    "# Use DataLoader:\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Shuffle the training data to prevent memorization.\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # Validation set doesn't need shuffling because we want consistent evaluation.\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Testing set doesn't need shuffling because we want consistent evaluation.\n",
    "\n",
    "# Check to see that our combinations are correct:\n",
    "print(\"Number of classes:\", len(train_dataset.classes))\n",
    "print(\"Sample class labels:\", train_dataset.classes[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d484fff",
   "metadata": {},
   "source": [
    "Looks good :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636d38e",
   "metadata": {},
   "source": [
    "We can write a generic training function for both models to simplify our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic model training function:\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # Keep track of our model's performance:\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode.\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            \n",
    "            # Clear previous gradients:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss:\n",
    "            running_loss = running_loss + loss.item()\n",
    "\n",
    "            # Calculate accuracy:\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = correct + (predicted == labels).sum().item()\n",
    "            total = total + labels.size(0)\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch:\n",
    "        avg_epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "\n",
    "        # Store metrics for plotting:\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        # Print progress:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f'Train Loss: {avg_epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60883294",
   "metadata": {},
   "source": [
    "Similarly, we can create a generic function that will evaluate the models' performance. This function will be used for both the validation and test sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4b0b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic model evaluation function:\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "\n",
    "    # Keep track of evaluation metrics:\n",
    "    evaluation_losses = []\n",
    "    evaluation_accuracies = []\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation because we aren't updating weights and biases.\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update loss:\n",
    "            running_loss = running_loss + loss.item()\n",
    "\n",
    "            # Calculate accuracy:\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = correct + (predicted == labels).sum().item()\n",
    "            total = total + labels.size(0)\n",
    "\n",
    "    # Calculate average evaluation loss and accuracy:\n",
    "    average_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Store metrics for plotting:\n",
    "    evaluation_losses.append(average_loss)\n",
    "    evaluation_accuracies.append(accuracy)\n",
    "\n",
    "    # Print results:\n",
    "    print(f\"Evaluation Loss: {average_loss:.4f}, Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return evaluation_losses, evaluation_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41100041",
   "metadata": {},
   "source": [
    "And finally, generic plotters for the losses and accuracies of the training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "567dbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(train_losses, train_accuracies, evaluation_losses, evaluation_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for loss:\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(epochs, evaluation_losses, label='Evaluation Loss', color='red', linestyle='-', marker='x')\n",
    "    plt.title('Model Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot for accuracy:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(epochs, evaluation_accuracies, label='Evaluation Accuracy', color='red', linestyle='-', marker='x')\n",
    "    plt.title('Model Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ec549",
   "metadata": {},
   "source": [
    "## **Multi-Layer Perceptron (Baseline Model):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29070f",
   "metadata": {},
   "source": [
    "For our baseline model, we will use a shallow multilayer perceptron. The general structure of a MLP model is as follows:\n",
    "\n",
    "- *Input Layer*: The first layer that receives the input data. Each node in this layer represents a feature of the input data.\n",
    "\n",
    "- *Hidden Layer(s)*: One or more layers between the input and output layers. Each node in these layers performs a weighted sum of inputs, applies an activation function, and passes the result to the next layer.\n",
    "\n",
    "- *Output Layer*: The final layer that produces the prediction or output of the model. The number of neurons in this layer corresponds to the number or classes for classification problems or a single value for regression problems.\n",
    "\n",
    "- *Weights & Biases*: Connections between neurons have associated weights, and each neuron has a bias. These parameters are learned during training to minimize the loss function.\n",
    "\n",
    "- *Activation Functions*: Nonlinear functions such as ReLU, Sigmoid, and Tanh are applied to the output of each neuron to introduce non-linearity, which allows the network to model complex relationships.\n",
    "\n",
    "An MLP is also fully-connected, meaning that each neuron in one layer is connected to every neuron in the next layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d881809",
   "metadata": {},
   "source": [
    "**Note:** Multilayer perceptrons require image-flattening into a 1-D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9578f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for MLP:\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create MLP model class:\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Keep the network shallow for now:\n",
    "        self.fc1 = nn.Linear(input_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # Flatten the image tensor into a 1-D vector.\n",
    "        x = torch.relu(self.fc1(x)) # Apply activation functions after each layer.\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # Output logits.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d1c2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model:\n",
    "MLP_model = MLP(input_dims=224*224*3, num_classes=len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56deb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "MLP_optimizer = optim.Adam(MLP_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49711aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train MLP:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_losses, train_accuracies = train_model(MLP_model, train_loader, criterion, MLP_optimizer, num_epochs=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Forward pass:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m outputs = model(images)\n\u001b[32m     23\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Backward pass:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cs171/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cs171/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     16\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# Flatten the image tensor into a 1-D vector.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     x = torch.relu(\u001b[38;5;28mself\u001b[39m.fc1(x)) \u001b[38;5;66;03m# Apply activation functions after each layer.\u001b[39;00m\n\u001b[32m     18\u001b[39m     x = torch.relu(\u001b[38;5;28mself\u001b[39m.fc2(x))\n\u001b[32m     19\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.fc3(x) \u001b[38;5;66;03m# Output logits.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cs171/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cs171/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cs171/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train MLP:\n",
    "MLP_train_losses, MLP_train_accuracies = train_model(MLP_model, train_loader, criterion, MLP_optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb06a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP on validation set:\n",
    "MLP_evaluation_losses, MLP_evaluation_accuracies = evaluate_model(MLP_model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6402b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the loss and accuracy:\n",
    "plot_metrics(MLP_train_losses, MLP_train_accuracies, MLP_evaluation_losses, MLP_evaluation_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ba4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP on test set: \n",
    "# DO NOT FILL IN THIS CODE CELL UNTIL AFTER HYPERPARAMETER TUNING!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff648b20",
   "metadata": {},
   "source": [
    "## **Convolutional Neural Network:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0a6fc",
   "metadata": {},
   "source": [
    "We expect that a CNN will perform much better overall than the MLP due to its ability to preserve spatial features. So, we will focus on tuning the CNN more. The general structure of a CNN is as follows:\n",
    "\n",
    "- *Input Layer*: The input layer takes in the raw data (usually images), represented as a grid of pixels (i.e., width * height * color channels = 32 * 32 * 3).\n",
    "\n",
    "- *Convolutional Layer(s)*: These layers apply convolutional filters (A.K.A. kernels) to the input or previous layer. Each filter detects specific features, such as edges or textures. These features are learned during training. The result is a set of feature maps, which highlight areas of the input that match the learned features.\n",
    "\n",
    "- *Activation Functions*: After convolution, an activation function (usually ReLU) is applied to introduce non-linearity. This helps the network learn more complex patterns.\n",
    "\n",
    "- *Pooling Layers*: Pooling layers (usually Max Pooling) reduce the spatial dimensions (width and height) of the feature maps. This helps reduce computation and makes the network more invariant to small translations of the input (i.e., shifting or zooming in an image). Essentially, pooling retains the most important features of the input data.\n",
    "\n",
    "- *Fully-Connected Layers*: After several convolutional and pooling layers, the data is flattened into a 1-D vector and passed through one or more fully-connected layers. These layers are similar to those in a traditional MLP and are used to make the final classification or regression prediction.\n",
    "\n",
    "- *Output Layer*: The final layer produces the output of the network. For classification tasks, this typically uses a softmax activation function (assuming multi-class classification) or sigmoid (assuming binary classification) to output probabilities.\n",
    "\n",
    "- *Weights and Biases*: Like an MLP, CNNs have weights and biases that are learned during training through backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea337a",
   "metadata": {},
   "source": [
    "**Note:** Convolutional neural networks do not require images to be flattened beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b83f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for CNN:\n",
    "\n",
    "# Create CNN model class:\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Keep the network shallow for now:\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64*32*32, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5382f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model:\n",
    "CNN_model = CNN(num_classes=len(train_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26c9e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "CNN_optimizer = optim.Adam(CNN_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befba668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN:\n",
    "CNN_train_losses, CNN_train_accuracies = train_model(CNN_model, train_loader, criterion, CNN_optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN on validation set:\n",
    "CNN_evaluation_losses, CNN_evaluation_accuracies = evaluate_model(CNN_model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the losses and accuracies:\n",
    "plot_metrics(CNN_evaluation_losses, CNN_evaluation_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b748e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN on test set: \n",
    "# DO NOT FILL IN THIS CODE CELL UNTIL AFTER HYPERPARAMETER TUNING!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
